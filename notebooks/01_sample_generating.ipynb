{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe051b04-276e-4ecc-9e65-d87f7c2b99ad",
   "metadata": {},
   "source": [
    "***************************************************************************************\n",
    "Jupyter Notebooks from the Metadata for Everyone project\n",
    "\n",
    "Code:\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "\n",
    "Project team: \n",
    "* Juan Pablo Alperin (https://orcid.org/0000-0002-9344-7439)\n",
    "* Dennis Donathan II (https://orcid.org/0000-0001-8042-0539)\n",
    "* Mike Nason (https://orcid.org/0000-0001-5527-8489)\n",
    "* Julie Shi (https://orcid.org/0000-0003-1242-1112)\n",
    "* Marco Tullney (https://orcid.org/0000-0002-5111-2788)\n",
    "\n",
    "Last updated: xxx\n",
    "***************************************************************************************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a745ac0a",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "\n",
    "Here we will utiilize the Crossref REST API to generate our random sample. We'll use the habanero library, a wrapper for the Crossref API, to make the process easier. More info on the package can be found here: https://github.com/sckott/habanero\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e39cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from habanero import Crossref, WorksContainer\n",
    "import pandas as pd\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "# Directories for storing the data\n",
    "data_dir = Path('../data')\n",
    "input_dir = data_dir / 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59db1a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data(doi: str, email: Optional[str | None]):\n",
    "    \"\"\"Request function to query Crossref API.\n",
    "\n",
    "    Args:\n",
    "        doi (str): The DOI of an item, used for querying Crossref API\n",
    "\n",
    "    Returns:\n",
    "        JSON: with r.status_code == 200, returns JSON response\n",
    "        None: r.status_code == 404 will return None as the resource was not found\n",
    "        function: r.status_code == 504 returns the function to retry the query\n",
    "    \"\"\"\n",
    "    base_url = 'https://doi.crossref.org/search/doi'\n",
    "    params = {'format': 'unixsd',\n",
    "             'doi': doi}\n",
    "    if email:\n",
    "        params['pid'] = email\n",
    "    try:\n",
    "        r = requests.get(base_url, params=params)\n",
    "        if r.status_code == 200:\n",
    "            soup = BeautifulSoup(r.content.decode('utf-8').replace('\\n', '').replace('\\r', ''), 'xml')\n",
    "            return  ''.join(str(tag) for tag in soup.find_all()).replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "        elif r.status_code == 404:\n",
    "            return None  \n",
    "        elif r.status_code == 504:\n",
    "            print(r.status_code)\n",
    "            time.sleep(1)\n",
    "            return fetch_data(doi)\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching DOI {doi}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def get_crossref(id_list: List[str]):\n",
    "    \"\"\"Primary function for querying Crossref API and collecting responses\n",
    "\n",
    "    Args:\n",
    "        id_list (list): List of all DOIs to be queried.\n",
    "    \"\"\"\n",
    "    chunk_size = 5000\n",
    "    tmp = []\n",
    "    \n",
    "    print(f\"Going after: {len(id_list)}.\")\n",
    "    \n",
    "    file_path = data_dir / 'doi_file.csv'\n",
    "    if file_path.is_file():\n",
    "        print(f\"The file {file_path} exists.\")\n",
    "        # cut -d',' -f1 doi_file.csv > dois_read.csv\n",
    "    else:\n",
    "        pd.DataFrame(columns=['DOI', 'message']).to_csv(file_path, mode='w', index=False)\n",
    "\n",
    "#     already_read = pd.read_csv(data_dir / 'dois_read.csv')\n",
    "#     print(f\"Already read: {len(already_read)}.\")\n",
    "#     id_list = list(set(id_list).difference(already_read.DOI.str.lower()))\n",
    "    print(f\"Going after: {len(id_list)}.\")\n",
    "        \n",
    "    # Record the starting time\n",
    "    start_time = time.time()\n",
    "    \n",
    "   \n",
    "    with tqdm(total=len(id_list)) as pbar:\n",
    "        for i, doi in enumerate(id_list):\n",
    "            try:\n",
    "                result = fetch_data(doi)\n",
    "                if result is not None:\n",
    "                    tmp.append({'DOI': doi, 'message': result})\n",
    "                    \n",
    "                if i % chunk_size == 0 or (i+1) == len(id_list):\n",
    "                    pd.DataFrame(tmp).to_csv(data_dir / 'doi_file.csv', mode='a', index=False, header=False)\n",
    "                    tmp = []\n",
    "                    end_time = time.time()\n",
    "                    if i/3 > (end_time - start_time):\n",
    "                        pause = i/3 - (end_time - start_time) \n",
    "                        print(f\"Sleeping: {int(pause)} seconds\")\n",
    "                        time.sleep(pause)\n",
    "\n",
    "                pbar.update(1)\n",
    "            except KeyboardInterrupt:\n",
    "                if len(tmp) > 1: \n",
    "                    pd.DataFrame(tmp).to_csv(data_dir / 'doi_file.csv', mode='a', index=False, header=False)                \n",
    "                raise\n",
    "            except Exception as err:                \n",
    "                print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824f242-8adb-4d72-81c3-6e94e29fa7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please note: If you take a new sample, the results will also change slightly. \n",
    "# To repeat our calculations, use our data sample. \n",
    "# To check our results with a new analysis, get a new sample."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dfd2b3",
   "metadata": {},
   "source": [
    "## Getting the full Sample\n",
    "The initial query looks good, so we'll move on to getting the full sample. We're looking for 500,000 unique records. This may take some time to collect, so best to run it overnight or in the background.\n",
    "\n",
    "We want 500,000 unique records, so we'll set up our loop to count the number of unique DOIs we have and stop once we have 500,000. We'll have duplicates, and we'll handle those in the data cleaning notebook (among other things).\n",
    "\n",
    "Since this can take a while, we'll want to build in a safety net against errors and timeouts. If an error occurs, the data is saved, the script is given some sleep time, then it begins again.\n",
    "\n",
    "Once it has hit 500,000 unique records, we'll save the file and move on to cleaning the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
